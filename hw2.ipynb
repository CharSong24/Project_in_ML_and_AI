{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b255075b-9d07-4316-ba38-773ddc1af74e",
   "metadata": {},
   "source": [
    "## Task 1 (30 points): Implement a Decision Tree Classifier for your classification problem. You may use a built-in package to implement your classifier. Try modifying one or more of the input parameters and describe what changes you notice in your results. Clearly describe how these factors are affecting your output."
   ]
  },
  {
   "cell_type": "raw",
   "id": "adfa132c-3481-49c1-9d15-f002b58fb669",
   "metadata": {},
   "source": [
    "Problem: Using logistic regression to predict the authenticity of banknotes.\n",
    "\n",
    "This problem is a typical binary classification issue, aiming to predict whether banknotes are genuine or counterfeit based on features extracted from images of the banknotes (such as the 'Variance of Wavelet Transformed Image', 'Skewness of Wavelet Transformed Image', 'Curtosis of Wavelet Transformed Image', and 'Entropy of Image'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4d228a-8563-433c-8c97-0fd869e574d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy initial:  0.9805825242718447\n",
      "accuracy modified:  0.9296116504854369\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load \n",
    "data = pd.read_csv('data_banknote_authentication.txt', header=None)\n",
    "data.columns = ['Variance of Wavelet Transformed Image', 'Skewness of Wavelet Transformed Image',\n",
    "                   'Curtosis of Wavelet Transformed Image', 'Entropy of Image', 'Target']\n",
    "\n",
    "\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_initial = accuracy_score(y_test, y_pred)\n",
    "\n",
    "clf_modified = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_modified.fit(X_train, y_train)\n",
    "y_pred_modified = clf_modified.predict(X_test)\n",
    "accuracy_modified = accuracy_score(y_test, y_pred_modified)\n",
    "\n",
    "print(\"accuracy initial: \", accuracy_initial, \"\\n\")\n",
    "print(\"accuracy modified: \",accuracy_modified)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1a4ea47-cd38-49e9-81ce-5d579c50772c",
   "metadata": {},
   "source": [
    "Change in resultï¼š When max_depth was set to 3, the accuracy of the model dropped from 98.06% to 92.96%. This suggests that limiting the depth of the decision tree may reduce the model's ability to capture complex patterns in the data.\n",
    "\n",
    "Analysis: \n",
    "Reduce overfitting: Reducing the depth of the tree is usually done to prevent the model from becoming overly complex, thereby reducing the risk of overfitting. This is an effective strategy when the training data is very complex or noisy.\n",
    "Generalization ability: Although simplifying a model may result in a slight decrease in performance on the training set, it usually improves the model's ability to generalize on unseen data.\n",
    "Balanced selection: Choosing the best max_depth is a balancing process, which requires finding a balance between the degree of fit to the training data and the generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a52df6-0ebd-4c57-a71d-d3401f3073fe",
   "metadata": {},
   "source": [
    "## Task 2 (30 points): From the Bagging and Boosting ensemble methods pick any one algorithm from each category. Implement both the algorithms using the same data. Use k-fold cross validation to find the effectiveness of both the models. Comment on the difference/similarity of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b5b956-3a08-47c8-bbaf-70077e3c0c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:  0.9934412355865863\n",
      "Gradient Boosting:  0.9956257272823441\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# load\n",
    "\n",
    "data = pd.read_csv('data_banknote_authentication.txt', header=None)\n",
    "data.columns = ['Variance of Wavelet Transformed Image', 'Skewness of Wavelet Transformed Image',\n",
    "                   'Curtosis of Wavelet Transformed Image', 'Entropy of Image', 'Target']\n",
    "\n",
    "\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "\n",
    "#  create model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# apply k\n",
    "k = 10 \n",
    "rf_cv_scores = cross_val_score(rf_model, X, y, cv=k)\n",
    "gb_cv_scores = cross_val_score(gb_model, X, y, cv=k)\n",
    "\n",
    "#  cal\n",
    "rf_cv_accuracy = np.mean(rf_cv_scores)\n",
    "gb_cv_accuracy = np.mean(gb_cv_scores)\n",
    "\n",
    "print(\"Random Forest: \", rf_cv_accuracy)\n",
    "print(\"Gradient Boosting: \", gb_cv_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c50cd8b4-8296-425a-b249-e9cbe08031bb",
   "metadata": {},
   "source": [
    "Similarities\n",
    "High Accuracy: Both models exhibit very high accuracy in the classification of genuine and counterfeit banknotes, indicating that they are both highly suitable for this type of problem.\n",
    "Effective Ensemble Strategy: Both methods are based on the principles of ensemble learning, which involves combining multiple learners to enhance predictive performance. This demonstrates the effectiveness of ensemble learning in handling complex classification problems.\n",
    "\n",
    "Differences\n",
    "Gradient Boosting (99.56%) slightly outperforms Random Forest (99.34%). This may suggest that Gradient Boosting has a better capability in handling samples that are more difficult to classify in this particular problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d2849-ddc3-48dc-83fc-94663babbb26",
   "metadata": {},
   "source": [
    "## Task 3 (40 points): Compare the effectiveness of the three models implemented above. Clearly describe the metric you are using for comparison. Describe (with examples) Why is this metric(metrics) suited/appropriate for the problem at hand? How would a choice of a different metric impact your results? Can you demonstrate that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774bcd70-0dc0-4afb-a4aa-9420b6bb6652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Precision: 0.9888268156424581 Recall: 0.9672131147540983\n",
      "Random Forest - Precision: 1.0 Recall: 0.994535519125683\n",
      "Gradient Boosting - Precision: 1.0 Recall: 0.994535519125683\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# load\n",
    "data = pd.read_csv('data_banknote_authentication.txt', header=None)\n",
    "data.columns = ['Variance of Wavelet Transformed Image', 'Skewness of Wavelet Transformed Image',\n",
    "                'Curtosis of Wavelet Transformed Image', 'Entropy of Image', 'Target']\n",
    "\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# cal\n",
    "precision_dt = precision_score(y_test, y_pred)\n",
    "recall_dt = recall_score(y_test, y_pred)\n",
    "\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "\n",
    "precision_gb = precision_score(y_test, y_pred_gb)\n",
    "recall_gb = recall_score(y_test, y_pred_gb)\n",
    "\n",
    "#\n",
    "print(\"Decision Tree - Precision:\", precision_dt, \"Recall:\", recall_dt)\n",
    "print(\"Random Forest - Precision:\", precision_rf, \"Recall:\", recall_rf)\n",
    "print(\"Gradient Boosting - Precision:\", precision_gb, \"Recall:\", recall_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35497b-d0f2-44a6-808d-14c371172880",
   "metadata": {},
   "source": [
    "### Clearly describe the metric you are using for comparison:\n",
    "\n",
    "Accuracy, Precision, and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335f77d-ec8a-4c8b-90ce-9b57d977de42",
   "metadata": {},
   "source": [
    "### Describe (with examples) Why is this metric(metrics) suited/appropriate for the problem at hand?\n",
    "These metrics are suitable for banknote authenticity classification problems because: in a binary classification problem, the accuracy rate provides a quick, intuitive overview of model performance. In the verification of bank notes, misclassifying genuine currency as counterfeit (false positive) or counterfeit currency as genuine (false negative) may lead to different degrees of consequences. Thus, accuracy and recall rates provide more detailed information about these two types of errors, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2e38e-5249-4700-896d-08f5c657191f",
   "metadata": {},
   "source": [
    "### How would a choice of a different metric impact your results? Can you demonstrate that?\n",
    "If our primary concern is to reduce false positives (such as avoiding mistakenly classifying genuine banknotes as counterfeit), then we should focus more on precision. A model with high precision might sacrifice some recall, but it performs better in ensuring that a minority of genuine banknotes are not incorrectly marked.\n",
    "\n",
    "If we are more concerned about reducing false negatives (such as ensuring that as many counterfeit banknotes as possible are detected), then we should focus more on recall. Such a model might have more false positives, but it can more effectively identify counterfeit banknotes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
